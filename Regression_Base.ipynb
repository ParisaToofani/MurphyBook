{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If these turning epochs do not move with our will today,\n",
    "### The spheres of time are not constant, do not grieve\n",
    "\n",
    "### Hafiz\n",
    "\n",
    "<img src=\"sinewave.jpg\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import typing as tp\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression from Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Simple Regression Model\n",
    "The most implementation that is done from scratch are based on the following book.\n",
    "\n",
    "*Grus, J. (2015). Data science from scratch: first principles with Python. First edition. Sebastopol, CA, O'Reilly.*\n",
    "\n",
    "The basic formulation of the linear regression is based on the intercept ($w_0$) and slope ($w_1$).\n",
    "\n",
    "$$y=w_0+w_1$$\n",
    "\n",
    "So, if we know the intercept and the slope, for every new data we can find the value of $y$ (Prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. The float in parentheses shows the input type is float.\n",
    "2. -> float: It shows the output is of type float\n",
    "\"\"\"\n",
    "def predict(w0: float, w1:float, x:float) -> float:\n",
    "    return w0+w1*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Multiple Regression\n",
    "\n",
    "Now, you should ask what if we had more variables that define my model?\n",
    "\n",
    "The answer is easy :) Just consider it as a vector and all multiplication that we have become like the element-wise vector multiplication.\n",
    "\n",
    "$$y_i = w_0 + w_1*x_{i1} + w_2*x_{i2} + ... + w_n*x_{in}$$\n",
    "$$ y = w^Tx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Implementation\n",
    "def predict(w:tp.List[float], x:tp.List[float]) -> tp.List[float]:\n",
    "    return w.T*x\n",
    "\n",
    "# A better implementation in the following link\n",
    "# https://github.com/oilneck/prml_python/blob/master/prml/fitting/bayesian_regression.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Least Square Estimation (Training Process)\n",
    "\n",
    "Lets see how we utilize the least square in our implementation!\n",
    "\n",
    "$$L(w)=\\frac{1}{2}||y-xw||^2_{2}$$\n",
    "\n",
    "Also, the quadratic format is:\n",
    "\n",
    "$$L(w)=\\frac{1}{2}(Xw-y)^T(Xw-y)$$\n",
    "\n",
    "We have to find the gradient of the $L(w)$ to find the weights.\n",
    "\n",
    "$$\\nabla L(w) = 0$$\n",
    "\n",
    "For finding the gradient we need to use some algorithms to find the roots of our required weights. There are different ways like Bisection method but here the best option is \"Gradient Descent\" algorithm.\n",
    "\n",
    "A Brief Review on the Gradient Descent Algorithm: \n",
    "\n",
    "Step 1 : make an initial guess\n",
    "Step 2 (general formulation):  $x_{i+1} = x_{i} - \\gamma_{i} ((\\nabla f)(x_i))^T$\n",
    "\n",
    "Some Tips:\n",
    "* If we choose a suitable step size, the sequence of $f(x_0)>f(x_1)>...$ that helps the algorithm to converge to the local minimum.\n",
    "* The step size is also called the learning rate.\n",
    "* If we choose the step size to be too small, the gradient descent can be too slow. On the other hand if we choose it to be too large, gradient descent can overshoot, fail to converge or even diverge :(. (One solution is adding the momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Goal: Performing the least square fit using gradient descent\n",
    "\"\"\"\n",
    "def second_norm_gradient(Xtrue: tp.List[float], Ytrue: tp.List[float], W: tp.List[float])-> tp.List[float]:\n",
    "    error = np.multiply(Xtrue, W) - Ytrue\n",
    "    \"\"\"\n",
    "    Error derivation based on linear algebra and notes\n",
    "    \"\"\"\n",
    "    second_norm_grad = [2*error*x for x in Xtrue]\n",
    "    return second_norm_grad\n",
    "\n",
    "def gradient_descent(guess, gradient, learning_rate):\n",
    "    \"\"\"\n",
    "    Exactly the formulation in the above description is utilized\n",
    "    \"\"\"\n",
    "    new_value = guess - gradient * learning_rate\n",
    "    return new_value\n",
    "def least_square_fit(Xdata: tp.List[float], Ydata: tp.List[float],\n",
    "                     learning_rate: float=0.001, iteration: int=1000)-> tp.List[float]:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    Xdata: The inputs of the dataset\n",
    "    Ydata: The actual output of the dataset\n",
    "    Learning Rate: Step Size\n",
    "    Iteration: The total number that we have to make a prediction (update the weights)\n",
    "    Epoch: Every time that model see the whole dataset\n",
    "    \"\"\"\n",
    "    # initial is our first guess for the weights to start the iteration\n",
    "    initial = [random.random for _ in range(int(Xdata.shape[1]))]\n",
    "    # Start Iteration\n",
    "    for _ in range(iteration):\n",
    "        for start in range(len(Xdata)):\n",
    "            # second_norm_gradient\n",
    "            sc_norm_gradient = second_norm_gradient(Xdata, Ydata, initial)\n",
    "            # gradient_descent\n",
    "            initial = gradient_descent(initial, sc_norm_gradient, learning_rate)\n",
    "    return initial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Regularization (First solution for the overfitting)\n",
    "1.4.1. Ridge Regression\n",
    "\n",
    "Ridge regression is an alternative for the Maximum likelihood estimation (MLE) as MLE can sometimes result in overfitting. What we do exactly in Ridge Regression is including our prior knowledge about data. How we define the prior? All the things are in your hand:))). Here, the Gaussian prior with zero mean is chosen based on our friend Murphy's book. The good thing about ridge regression is that the least square optimization using the gradient descent works for that. (We have our prior, does it remind you some method that we used it before?)\n",
    "\n",
    "$$\\hat{w}_{MAP} = argmin \\frac{1}{2\\sigma^2}(y-Xw)^T(y-Xw)+\\frac{1}{2\\tau^2}w^Tw$$\n",
    "$$\\hat{w}_{MAP} = argmin RSS(w)+\\lambda||w||^2_2$$\n",
    "\n",
    "1.4.2. Lasso Regression\n",
    "\n",
    "This is another approach to overcome the overfitting problem, I myself believe that the ability of Lasso in feature selection is more important than overcoming the overfitting the peoblem. In Lasso, we actually minimize the $L_0$ Norm.\n",
    "$$||w_0|| = \\sum^{D}_{d=1} I(|w_d|>0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Goal: \n",
    "1. Define the Ridge penalty and find the gradient of the function\n",
    "2. Define the least square function for the ridge regression\n",
    "3. Define the Lasso penalty\n",
    "\"\"\"\n",
    "def ridge_penalty(lamda: float, w:tp.List[float])->float:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    Lambda: The prior strength coefficient\n",
    "    w: model weights\n",
    "    \"\"\"\n",
    "    ridge_penalt = lamda * np.dot(w,w)\n",
    "    return ridge_penalt\n",
    "\n",
    "def ridge_gradient(lamda: float, w:tp.List[float])-> tp.List[float]:\n",
    "    \"\"\"\n",
    "    Simple derivatives calculation\n",
    "    \"\"\"\n",
    "    ridge_grad = [2 * lamda * w_i for w_i in w]\n",
    "    return ridge_grad\n",
    "\n",
    "def least_square_fit_ridge(Xdata: tp.List[float], Ydata: tp.List[float], lamda: float=0,\n",
    "                     learning_rate: float=0.001, iteration: int=1000)-> tp.List[float]:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    Xdata: The inputs of the dataset\n",
    "    Ydata: The actual output of the dataset\n",
    "    Learning Rate: Step Size\n",
    "    Iteration: The total number that we have to make a prediction (update the weights)\n",
    "    Epoch: Every time that model see the whole dataset\n",
    "    \"\"\"\n",
    "    # initial is our first guess for the weights to start the iteration\n",
    "    initial = [random.random for _ in range(int(Xdata.shape[1]))]\n",
    "    # Start Iteration\n",
    "    for _ in range(iteration):\n",
    "        for start in range(len(Xdata)):\n",
    "            # second_norm_gradient\n",
    "            sc_norm_gradient = second_norm_gradient(Xdata, Ydata, initial) + ridge_gradient(lamda, initial)\n",
    "            # gradient_descent\n",
    "            initial = gradient_descent(initial, sc_norm_gradient, learning_rate)\n",
    "    return initial\n",
    "\n",
    "def lasso_penalty(lamda: float, w:tp.List[float])->float:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    Lambda: The prior strength coefficient\n",
    "    w: model weights\n",
    "    \"\"\"\n",
    "    # !!! Modify for the absolute values\n",
    "    lasso_penalt = lamda * np.sum(w)\n",
    "    return lasso_penalt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Goodness of Fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Advanced Models Using the Regression Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "```\n",
    "\n",
    "The codes implemented here are based on the two following references which follows Object Oriented Programming (OOP). Therefore, it would be a good idea to reviewing the following source, even if you are familiar eith OOP.\n",
    "\n",
    "Source: https://realpython.com/python3-object-oriented-programming/\n",
    "\n",
    "Refrences:\n",
    "\n",
    "1. Bishop, Christopher M. (2006). Pattern recognition and machine learning. New York :Springer,\n",
    "\n",
    "2. https://github.com/oilneck/prml_python\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Linear Basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyTransformer(object):\n",
    "    # The main input that is necessary to initialize the model\n",
    "    #  with that is the degree of Polynomial function\n",
    "    def __init__(self, degree=1):\n",
    "        self.degree = degree\n",
    "    def transform(self, Xtrain: np.ndarray)-> np.ndarray:\n",
    "        Xtrain = Xtrain.reshape(-1,1)\n",
    "        if Xtrain.shape[1] == 1:\n",
    "            Xtrain = Xtrain.T\n",
    "        assert Xtrain.shape[1] == self.degree + 1\n",
    "        x_train_transform = np.power(Xtrain, np.arange(0, self.degree+1))\n",
    "        return x_train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basis(Xtrain: np.ndarray, basis: np.ndarray, basis_type: str):\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = (10, 10)\n",
    "    plt.rcParams[\"font.family\"] = \"Serif\"\n",
    "    plt.rcParams['xtick.labelsize']=12\n",
    "    plt.rcParams['ytick.labelsize']=12\n",
    "    plt.plot(Xtrain, basis)\n",
    "    plt.title('Basis of type %s' % basis_type)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Basis Values')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m xtrain\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m)\n\u001b[0;32m      5\u001b[0m xtrain\u001b[39m.\u001b[39mshape\n\u001b[1;32m----> 6\u001b[0m polybasis \u001b[39m=\u001b[39m PolyTransformer(degree\u001b[39m=\u001b[39;49m\u001b[39m11\u001b[39;49m)\u001b[39m.\u001b[39;49mtransform(xtrain)\n",
      "Cell \u001b[1;32mIn[51], line 10\u001b[0m, in \u001b[0;36mPolyTransformer.transform\u001b[1;34m(self, Xtrain)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mif\u001b[39;00m Xtrain\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      9\u001b[0m     Xtrain \u001b[39m=\u001b[39m Xtrain\u001b[39m.\u001b[39mT\n\u001b[1;32m---> 10\u001b[0m \u001b[39massert\u001b[39;00m Xtrain\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegree \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     11\u001b[0m x_train_transform \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpower(Xtrain, np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegree\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m x_train_transform\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform a test on the developed modules\n",
    "\"\"\"\n",
    "xtrain=np.linspace(-1, 1, 100)\n",
    "xtrain.shape\n",
    "polybasis = PolyTransformer(degree=11).transform(xtrain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Polynomial Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Bayesian Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. General Perspective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Feeding New Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Bayesian Curve Fitting "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLProject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
