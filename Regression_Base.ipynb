{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science and Machine Learning is *interesting* but understanding the background of the is *much more interesting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import typing as tp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression from Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Simple Regression Model\n",
    "The most implementation that is done from scratch are based on the following book.\n",
    "\n",
    "*Grus, J. (2015). Data science from scratch: first principles with Python. First edition. Sebastopol, CA, O'Reilly.*\n",
    "\n",
    "The basic formulation of the linear regression is based on the intercept ($w_0$) and slope ($w_1$).\n",
    "\n",
    "$$y=w_0+w_1$$\n",
    "\n",
    "So, if we know the intercept and the slope, for every new data we can find the value of $y$ (Prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. The float in parentheses shows the input type is float.\n",
    "2. -> float: It shows the output is of type float\n",
    "\"\"\"\n",
    "def predict(w0: float, w1:float, x:float) -> float:\n",
    "    return w0+w1*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Multiple Regression\n",
    "\n",
    "Now, you should ask what if we had more variables that define my model?\n",
    "\n",
    "The answer is easy :) Just consider it as a vector and all multiplication that we have become like the element-wise vector multiplication.\n",
    "\n",
    "$$y_i = w_0 + w_1*x_{i1} + w_2*x_{i2} + ... + w_n*x_{in}$$\n",
    "$$ y = w^Tx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w:tp.List[float], x:tp.List[float]) -> tp.List[float]:\n",
    "    return w*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Least Square Estimation (Training Process)\n",
    "\n",
    "Lets see how we utilize the least square in our implementation!\n",
    "\n",
    "$$L(w)=\\frac{1}{2}||y-xw||^2_{2}$$\n",
    "\n",
    "Also, the quadratic format is:\n",
    "\n",
    "$$L(w)=\\frac{1}{2}(Xw-y)^T(Xw-y)$$\n",
    "\n",
    "We have to find the gradient of the $L(w)$ to find the weights.\n",
    "\n",
    "$$\\nabla L(w) = 0$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Advanced Models Using the Regression Method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLProject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
