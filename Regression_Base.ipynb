{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If these turning epochs do not move with our will today,\n",
    "### The spheres of time are not constant, do not grieve\n",
    "\n",
    "### Hafiz\n",
    "\n",
    "<img src=\"sinewave.jpg\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import typing as tp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression from Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Simple Regression Model\n",
    "The most implementation that is done from scratch are based on the following book.\n",
    "\n",
    "*Grus, J. (2015). Data science from scratch: first principles with Python. First edition. Sebastopol, CA, O'Reilly.*\n",
    "\n",
    "The basic formulation of the linear regression is based on the intercept ($w_0$) and slope ($w_1$).\n",
    "\n",
    "$$y=w_0+w_1$$\n",
    "\n",
    "So, if we know the intercept and the slope, for every new data we can find the value of $y$ (Prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. The float in parentheses shows the input type is float.\n",
    "2. -> float: It shows the output is of type float\n",
    "\"\"\"\n",
    "def predict(w0: float, w1:float, x:float) -> float:\n",
    "    return w0+w1*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Multiple Regression\n",
    "\n",
    "Now, you should ask what if we had more variables that define my model?\n",
    "\n",
    "The answer is easy :) Just consider it as a vector and all multiplication that we have become like the element-wise vector multiplication.\n",
    "\n",
    "$$y_i = w_0 + w_1*x_{i1} + w_2*x_{i2} + ... + w_n*x_{in}$$\n",
    "$$ y = w^Tx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w:tp.List[float], x:tp.List[float]) -> tp.List[float]:\n",
    "    return w*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Least Square Estimation (Training Process)\n",
    "\n",
    "Lets see how we utilize the least square in our implementation!\n",
    "\n",
    "$$L(w)=\\frac{1}{2}||y-xw||^2_{2}$$\n",
    "\n",
    "Also, the quadratic format is:\n",
    "\n",
    "$$L(w)=\\frac{1}{2}(Xw-y)^T(Xw-y)$$\n",
    "\n",
    "We have to find the gradient of the $L(w)$ to find the weights.\n",
    "\n",
    "$$\\nabla L(w) = 0$$\n",
    "\n",
    "For finding the gradient we need to use some algorithms to find the roots of our required weights. There are different ways like Bisection method but here the best option is \"Gradient Descent\" algorithm.\n",
    "\n",
    "A Brief Review on the Gradient Descent Algorithm: \n",
    "\n",
    "Step 1 : make an initial guess\n",
    "Step 2 (general formulation):  $x_{i+1} = x_{i} - \\gamma_{i} ((\\nabla f)(x_i))^T$\n",
    "\n",
    "Some Tips:\n",
    "* If we choose a suitable step size, the sequence of $f(x_0)>f(x_1)>...$ that helps the algorithm to converge to the local minimum.\n",
    "* The step size is also called the learning rate.\n",
    "* If we choose the step size to be too small, the gradient descent can be too slow. On the other hand if we choose it to be too large, gradient descent can overshoot, fail to converge or even diverge :(. (One solution is adding the momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Goal: Performing the least square fit using gradient descent\n",
    "\"\"\"\n",
    "def second_norm_gradient(Xtrue: tp.List[float], Ytrue: tp.List[float], W: tp.List[float])-> tp.List[float]:\n",
    "    error = np.multiply(Xtrue, W) - Ytrue\n",
    "    \"\"\"\n",
    "    Error derivation based on linear algebra and notes\n",
    "    \"\"\"\n",
    "    second_norm_grad = [2*error*x for x in Xtrue]\n",
    "    return second_norm_grad\n",
    "\n",
    "def gradient_descent(guess, gradient, learning_rate):\n",
    "    \"\"\"\n",
    "    Exactly the formulation in the above description is utilized\n",
    "    \"\"\"\n",
    "    new_value = guess - gradient * learning_rate\n",
    "    return new_value\n",
    "def least_square_fit(Xdata: tp.List[float], Ydata: tp.List[float],\n",
    "                     learning_rate: float=0.001, iteration: int=1000)-> tp.List[float]:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    Xdata: The inputs of the dataset\n",
    "    Ydata: The actual output of the dataset\n",
    "    Learning Rate: Step Size\n",
    "    Iteration: The total number that we have to make a prediction (update the weights)\n",
    "    Epoch: Every time that model see the whole dataset\n",
    "    \"\"\"\n",
    "    # Start Iteration\n",
    "    for _ in range(iteration):\n",
    "        for start in range(len(Xdata)):\n",
    "            # second_norm_gradient\n",
    "            sc_norm_gradient = second_norm_gradient(Xdata, Ydata, initial)\n",
    "            # gradient_descent\n",
    "            initial = gradient_descent(initial, sc_norm_gradient, learning_rate)\n",
    "    return initial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Advanced Models Using the Regression Method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLProject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
